# @package _global_

defaults:
  - override /data: default
  - override /training: default
  - override /model: unet

data:
  batch_size: ${BATCH_SIZE}
  num_workers: ${NUM_WORKERS}

training:
  optimizer:
    lr: ${LR}
    _target_: torch.optim.${OPTIMIZER}
  precision: ${PRECISION}
  max_epochs: 15

wandb:
  enabled: true
  tags: ["sweep", "hyperparameter_tuning"]
